# Optimization-Methods-in-Neural-Networks
Reimplementation and exploration of the Lion optimization method, as described in the original paper "Lion," and its comparative analysis with several widely-used optimization algorithms, including AdamW, Stochastic Gradient Descent (SGD) with momentum, Nesterov Accelerated Gradient (Nesterov AG), AdaGrad, and RMSProp.
